{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00d77811",
   "metadata": {},
   "source": [
    "# (PART) DATA EXPLORATION {-}\n",
    "# How do you create a project directory ready for analysis?\n",
    "\n",
    "## Explanation  \n",
    "Before working with data, it's important to set up a clean and organized project directory. A consistent folder structure helps you manage scripts, datasets, and outputs across both Python and R â€” making your work easier to follow and share.\n",
    "\n",
    "In this guide, weâ€™ll create a root directory called `general-data-science` with four folders:\n",
    "\n",
    "- `data/` â€“ for datasets  \n",
    "- `scripts/` â€“ for code files  \n",
    "- `images/` â€“ for plots and charts  \n",
    "- `library/` â€“ for reusable functions  \n",
    "\n",
    "---\n",
    "\n",
    "**Example Folder Structure:**\n",
    "\n",
    "```plaintext\n",
    "general-data-science/\n",
    "â”œâ”€â”€ data/\n",
    "â”œâ”€â”€ scripts/\n",
    "â”œâ”€â”€ images/\n",
    "â””â”€â”€ library/\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef86866",
   "metadata": {},
   "source": [
    "## Bash (Terminal)\n",
    "\n",
    "You can create the entire structure using this single command:\n",
    "\n",
    "```bash\n",
    "mkdir -p general-data-science/{data,scripts,images,library}\n",
    "cd general-data-science\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff63dca6",
   "metadata": {},
   "source": [
    "## Python Code\n",
    "\n",
    "You can also create the same folder structure in Python:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9ee3ab",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "\n",
    "folders = [\"data\", \"scripts\", \"images\", \"library\"]\n",
    "root = \"general-data-science\"\n",
    "\n",
    "os.makedirs(root, exist_ok=True)\n",
    "for folder in folders:\n",
    "    os.makedirs(os.path.join(root, folder), exist_ok=True)\n",
    "\n",
    "print(f\"Created '{root}' project folder with subdirectories.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6b46d2",
   "metadata": {},
   "source": [
    "## R Code\n",
    "\n",
    "Hereâ€™s how to do it in R:\n",
    "\n",
    "```R\n",
    "folders <- c(\"data\", \"scripts\", \"images\", \"library\")\n",
    "root <- \"general-data-science\"\n",
    "\n",
    "if (!dir.exists(root)) dir.create(root)\n",
    "for (folder in folders) {\n",
    "  dir.create(file.path(root, folder), showWarnings = FALSE)\n",
    "}\n",
    "\n",
    "cat(\"Created\", root, \"project folder with subdirectories.\\n\")\n",
    "```\n",
    "\n",
    "> âœ… A clean project directory helps you stay organized, reuse code, and avoid errors â€” itâ€™s the first step toward reproducible, professional data science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9586d22d",
   "metadata": {},
   "source": [
    "# How do you install basic data science tools and libraries for Python and R?\n",
    "\n",
    "## Explanation  \n",
    "Before you can analyze data in Python or R, you need to install essential libraries. These libraries provide tools for **data manipulation**, **visualization**, **statistical analysis**, and **machine learning** â€” the four core layers in the CDI learning system. Installing them ensures you're ready to explore datasets and build reproducible workflows.\n",
    "\n",
    "---\n",
    "\n",
    "## Python Tools\n",
    "\n",
    "In your terminal or command prompt, run:\n",
    "\n",
    "```bash\n",
    "pip install pandas numpy matplotlib seaborn scikit-learn scipy\n",
    "```\n",
    "\n",
    "Then, import and check versions to confirm installation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea33a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import scipy\n",
    "\n",
    "from scipy import stats  # still useful to import for use\n",
    "\n",
    "print(\"pandas:\", pd.__version__)\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"matplotlib:\", plt.matplotlib.__version__)\n",
    "print(\"seaborn:\", sns.__version__)\n",
    "print(\"scikit-learn:\", sklearn.__version__)\n",
    "print(\"scipy:\", scipy.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01cbd96",
   "metadata": {},
   "source": [
    "**Installed Python libraries by layer:**\n",
    "\n",
    "- ðŸ§¹ **EDA**:  \n",
    "  - `pandas` â€“ Tabular data structures and data cleaning tools  \n",
    "  - `numpy` â€“ Efficient array operations for numerical computing  \n",
    "\n",
    "- ðŸ“Š **Visualization**:  \n",
    "  - `matplotlib` â€“ Customizable static and interactive plots  \n",
    "  - `seaborn` â€“ Statistical data visualizations built on matplotlib  \n",
    "\n",
    "- ðŸ“ **Statistical Analysis**:  \n",
    "  - `scipy.stats` â€“ Tools for distributions, t-tests, ANOVA, correlation, and more  \n",
    "\n",
    "- ðŸ¤– **Machine Learning**:  \n",
    "  - `scikit-learn` â€“ Algorithms and utilities for classification, regression, clustering, model evaluation  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523de0a3",
   "metadata": {},
   "source": [
    "## R Tools\n",
    "\n",
    "```{r eval=FALSE, echo=TRUE}\n",
    "# -----------------------------\n",
    "# ðŸ“Š EDA (Exploratory Data Analysis)\n",
    "# -----------------------------\n",
    "if (!require(tidyverse)) install.packages(\"tidyverse\")\n",
    "library(tidyverse)\n",
    "\n",
    "# -----------------------------\n",
    "# ðŸ“ˆ Visualization\n",
    "# -----------------------------\n",
    "if (!require(GGally)) install.packages(\"GGally\")\n",
    "library(GGally)\n",
    "\n",
    "# -----------------------------\n",
    "# ðŸ“ Statistical Analysis (STATS)\n",
    "# -----------------------------\n",
    "if (!require(broom)) install.packages(\"broom\")\n",
    "library(broom)\n",
    "\n",
    "if (!require(car)) install.packages(\"car\")\n",
    "library(car)\n",
    "\n",
    "if (!require(emmeans)) install.packages(\"emmeans\")\n",
    "library(emmeans)\n",
    "\n",
    "# -----------------------------\n",
    "# ðŸ¤– Machine Learning\n",
    "# -----------------------------\n",
    "if (!require(caret)) install.packages(\"caret\")\n",
    "library(caret)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122c7619",
   "metadata": {},
   "source": [
    "**Installed R packages by layer:**\n",
    "\n",
    "- ðŸ§¹ **EDA**:  \n",
    "  - `tidyverse` â€“ A collection of packages for tidy data workflows:  \n",
    "    - `dplyr` (data manipulation)  \n",
    "    - `readr` (reading CSV and text files)  \n",
    "    - `tidyr` (reshaping data)  \n",
    "    - `tibble` (modern data frames)  \n",
    "    - `stringr` (string operations)  \n",
    "    - `forcats` (working with factors)  \n",
    "    - `ggplot2` (visualization)  \n",
    "    - `purrr` (functional programming)\n",
    "\n",
    "- ðŸ“Š **Visualization**:  \n",
    "  - `ggplot2` â€“ Grammar of graphics for elegant visualizations (included in tidyverse)  \n",
    "  - `GGally` â€“ Enhances ggplot2 with matrix plots, correlation plots, etc.  \n",
    "\n",
    "- ðŸ“ **Statistical Analysis**:  \n",
    "  - `broom` â€“ Converts model outputs into tidy data frames  \n",
    "  - `car` â€“ Tools for regression diagnostics, ANOVA, linear models  \n",
    "  - `emmeans` â€“ Estimated marginal means for post-hoc testing and comparisons  \n",
    "\n",
    "- ðŸ¤– **Machine Learning**:  \n",
    "  - `caret` â€“ A unified framework for training, tuning, and comparing models across many algorithms  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f04aacd",
   "metadata": {},
   "source": [
    "> âœ… Once these tools are installed, youâ€™ll be ready to acquire datasets and begin your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34995e",
   "metadata": {},
   "source": [
    "# What are common sources of datasets for Python and R?\n",
    "\n",
    "## Explanation\n",
    "\n",
    "Before working with data, itâ€™s important to know **where data comes from**. In both Python and R, you can use:\n",
    "\n",
    "1. **Public datasets** from libraries or platforms  \n",
    "2. **Downloaded datasets** from repositories  \n",
    "3. **Real-world data** from research, surveys, APIs, or government sources\n",
    "\n",
    "These sources help you practice data skills using real, structured information.\n",
    "\n",
    "**Common sources include:**\n",
    "\n",
    "- **Built-in datasets**:  \n",
    "  - Python: `seaborn`, `sklearn.datasets`, `statsmodels`, `pydataset`  \n",
    "  - R: `datasets` package, `MASS`, `ggplot2`, `palmerpenguins`\n",
    "\n",
    "\n",
    "- **Online repositories**:  \n",
    "  - [UCI Machine Learning Repository](https://archive.ics.uci.edu/)  \n",
    "  - [Kaggle Datasets](https://www.kaggle.com/datasets)  \n",
    "  - [data.gov](https://www.data.gov/)  \n",
    "  - [WHO, UN, World Bank](https://data.worldbank.org/)\n",
    "\n",
    "\n",
    "- **Research & Surveys**:  \n",
    "  - CSV/Excel/JSON files published with academic papers or institutions  \n",
    "  - Survey data from organizations (e.g., Pew Research, Eurostat)\n",
    "\n",
    "- **APIs and live feeds**:  \n",
    "  - Weather, financial markets, genomics, social media (e.g., Twitter API)\n",
    "\n",
    "- **Local files**:  \n",
    "  - Saved from tools like Excel, Google Sheets, SPSS, or exported from databases\n",
    "\n",
    "## Python Package-Based Datasets\n",
    "\n",
    "- **Seaborn**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623375cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "print(iris[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386ebf26",
   "metadata": {},
   "source": [
    "- **Scikit-learn**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4604f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "irisml = datasets.load_iris()\n",
    "print(irisml.data[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4091ef",
   "metadata": {},
   "source": [
    "- **Statsmodels**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc59a62",
   "metadata": {},
   "source": [
    "import statsmodels.api as sm\n",
    "df = sm.datasets.get_rdataset(\"Guerry\", \"HistData\").data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d66130",
   "metadata": {},
   "source": [
    "\n",
    "## R Package-Based Datasets\n",
    "\n",
    "- **datasets** package:\n",
    "\n",
    "  ```{r}\n",
    "  iris <- datasets::iris\n",
    "  head(iris)\n",
    "  ```\n",
    "\n",
    "- **ggplot2**:\n",
    "\n",
    "  ```{r}\n",
    "  data(\"diamonds\", package = \"ggplot2\")\n",
    "  head(diamonds)\n",
    "  ```\n",
    "\n",
    "- **palmerpenguins** (if installed):\n",
    "\n",
    "  ```{r}\n",
    "  library(palmerpenguins)\n",
    "  head(penguins)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## Online Public Data Sources\n",
    "\n",
    "| Source                       | Link                                                |\n",
    "|-----------------------------|-----------------------------------------------------|\n",
    "| UCI Machine Learning Repo   | https://archive.ics.uci.edu/ml/                     |\n",
    "| Kaggle Datasets             | https://www.kaggle.com/datasets                     |\n",
    "| data.gov (US Government)    | https://www.data.gov                                |\n",
    "| Awesome Public Datasets     | https://github.com/awesomedata/awesome-public-datasets |\n",
    "| World Bank Open Data        | https://data.worldbank.org/                         |\n",
    "\n",
    "> ðŸ’¡ Tip: Always save downloaded datasets in your `data/` folder and reference them using relative paths like `data/filename.csv`.\n",
    "\n",
    "---\n",
    "\n",
    "> âœ… Now that you know where to find data, letâ€™s learn how to save, load and preview it in your Python or R environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560e075c",
   "metadata": {},
   "source": [
    "# How do you save a dataset in Python and R?\n",
    "\n",
    "## Explanation\n",
    "\n",
    "Once you've cleaned or prepared a dataset, it's good practice to save it in a standard format like CSV. This allows you to:\n",
    "\n",
    "- Preserve your cleaned version for future use\n",
    "- Avoid repeating preprocessing steps\n",
    "- Share your data with others or load it in different tools\n",
    "\n",
    "In this example, we'll use sample datasets provided by libraries in Python and R, then save them into the `data/` folder using `to_csv()` in Python and `write_csv()` in R.\n",
    "\n",
    "---\n",
    "\n",
    "## Python Code\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18200bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Create data folder\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Save seaborn's iris dataset\n",
    "df_iris_seaborn = sns.load_dataset(\"iris\")\n",
    "df_iris_seaborn.to_csv(\"data/iris_seaborn.csv\", index=False)\n",
    "\n",
    "print(\"\\nSeaborn Iris\\n\", df_iris_seaborn.head())\n",
    "\n",
    "\n",
    "# Save sklearn iris as well (optional)\n",
    "iris_sklearn = datasets.load_iris(as_frame=True).frame\n",
    "iris_sklearn.to_csv(\"data/iris_sklearn.csv\", index=False)\n",
    "\n",
    "print(\"\\nSklearn Iris\\n\", iris_sklearn.head())\n",
    "\n",
    "print(\"Datasets saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d779426e",
   "metadata": {},
   "source": [
    "## R Code\n",
    "\n",
    "```{r}\n",
    "# Load necessary libraries\n",
    "library(readr)\n",
    "library(datasets)\n",
    "\n",
    "# Create 'data/' directory if it doesn't exist\n",
    "if (!dir.exists(\"data\")) dir.create(\"data\")\n",
    "\n",
    "# Save the built-in iris dataset\n",
    "write_csv(iris, \"data/iris_rbase.csv\")\n",
    "\n",
    "cat(\"Datasets saved successfully.\\n\")\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "> âœ… After saving your cleaned or example dataset, you can now load it for further analysis or visualization in future sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fded3f5a",
   "metadata": {},
   "source": [
    "# How do you load a pre-cleaned dataset in Python and R?\n",
    "\n",
    "## Explanation\n",
    "\n",
    "Loading a dataset is one of the first steps in any data analysis project â€” especially when you're working from a previously saved, cleaned version.\n",
    "\n",
    "In this guide, we assume that the dataset `*.csv` has been saved in your `data/` folder. Weâ€™ll now load it using:\n",
    "\n",
    "- **Python**: via the `pandas` library and its `read_csv()` function\n",
    "- **R**: using the `readr` package and its `read_csv()` function\n",
    "\n",
    "Using consistent file paths (like `data/*.csv`) ensures reproducibility across environments.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Python Code\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259f76f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the pre-cleaned iris dataset\n",
    "df = pd.read_csv(\"data/iris_seaborn.csv\")\n",
    "\n",
    "# Preview the data\n",
    "print(df.head())\n",
    "\n",
    "# Confirm the shape\n",
    "print(\"Rows and columns:\", df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efc6e2d",
   "metadata": {},
   "source": [
    "## R Code\n",
    "\n",
    "```{r}\n",
    "library(readr)\n",
    "\n",
    "# Load the pre-cleaned iris dataset\n",
    "df <- read_csv(\"data/iris_rbase.csv\")\n",
    "\n",
    "# Preview the data\n",
    "head(df)\n",
    "\n",
    "# Dimensions\n",
    "cat(\"Rows and columns:\", dim(df), \"\\n\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "> âœ… Once loaded, you're ready to continue with data wrangling, visualization, or modeling â€” all based on your clean, reusable dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a41db9",
   "metadata": {},
   "source": [
    "# How do you rename column names in Python and R?\n",
    "\n",
    "## Explanation\n",
    "\n",
    "When working with the same dataset in both Python and R, you may encounter slight differences in column names â€” such as capitalization or spacing. To avoid confusion and ensure consistency across your analysis, itâ€™s best to standardize the column names.\n",
    "\n",
    "In this guide, weâ€™ll rename the columns to lowercase with underscores:\n",
    "\n",
    "- `sepal_length`  \n",
    "- `sepal_width`  \n",
    "- `petal_length`  \n",
    "- `petal_width`  \n",
    "- `species`\n",
    "\n",
    "After renaming, weâ€™ll save the final, standardized dataset as `data/iris.csv`, which will be used consistently throughout the rest of the guide.\n",
    "\n",
    "---\n",
    "\n",
    "## Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d84e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Using seaborn version of the iris dataset\n",
    "df1 = pd.read_csv(\"data/iris_seaborn.csv\")\n",
    "print(\"Original column names from seaborn version:\", df1.columns.tolist())\n",
    "\n",
    "# Rename columns\n",
    "df1.columns = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species\"]\n",
    "\n",
    "# Save standardized version\n",
    "df1.to_csv(\"data/iris.csv\", index=False)\n",
    "print(\"Saved standardized dataset from seaborn version as 'data/iris.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d71de4a",
   "metadata": {},
   "source": [
    "## R Code\n",
    "```{r}\n",
    "library(readr)\n",
    "library(dplyr)\n",
    "\n",
    "# Option 1: If your dataset already has lowercase column names (e.g., from iris_rbase.csv)\n",
    "df <- read_csv(\"data/iris_rbase.csv\")\n",
    "\n",
    "# Set standardized column names directly\n",
    "colnames(df) <- c(\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species\")\n",
    "\n",
    "# Save standardized dataset\n",
    "write_csv(df, \"data/iris.csv\")\n",
    "cat(\"Saved standardized dataset from iris_rbase.csv as data/iris.csv\\n\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7385dca7",
   "metadata": {},
   "source": [
    "```{r}\n",
    "# Option 2: If your dataset has capitalized column names (e.g., from iris_rbase.csv)\n",
    "df <- read_csv(\"data/iris_rbase.csv\")\n",
    "\n",
    "# Rename columns using dplyr for consistency\n",
    "df <- df %>%\n",
    "  rename(\n",
    "    sepal_length = Sepal.Length,\n",
    "    sepal_width  = Sepal.Width,\n",
    "    petal_length = Petal.Length,\n",
    "    petal_width  = Petal.Width,\n",
    "    species      = Species\n",
    "  )\n",
    "\n",
    "# Save standardized dataset\n",
    "write_csv(df, \"data/iris.csv\")\n",
    "cat(\"Saved standardized dataset from iris_rbase.csv as data/iris.csv\\n\")\n",
    "```\n",
    "\n",
    "\n",
    "> âœ… From this point forward, weâ€™ll use `data/iris.csv` as the **unified, clean dataset** for all Python and R examples in the guide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89e5dfc",
   "metadata": {},
   "source": [
    "# How do you examine the structure and types of variables in Python and R?\n",
    "\n",
    "## Explanation\n",
    "\n",
    "Understanding the **structure** of your dataset â€” including data types â€” is a key step in exploratory data analysis. It helps you:\n",
    "\n",
    "- Know what transformations are needed  \n",
    "- Identify categorical vs. numerical variables  \n",
    "- Prepare your data for modeling or visualization\n",
    "\n",
    "Each column in your dataset has a specific **data type**. These types influence how operations behave, how memory is allocated, and how functions treat your data.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Common Data Types in Python and R {-}\n",
    "\n",
    "| Concept            | Python (`pandas`)      | R (`base`)           | Notes |\n",
    "|:---------------|:------------------------|:-------------------|:--------------------------------------|\n",
    "| Integer            | `int`                  | `integer`            | Use `astype(int)` or `as.integer()` |\n",
    "| Decimal Number     | `float`                | `numeric`, `double`  | `numeric` in R defaults to `double` |\n",
    "| Text / String      | `str`, `object`        | `character`          | Use `astype(str)` or `as.character()` |\n",
    "| Logical / Boolean  | `bool`                 | `logical`            | `True`/`False` in Python, `TRUE`/`FALSE` in R |\n",
    "| Date / Time        | `datetime64[ns]`       | `Date`, `POSIXct`    | Use `pd.to_datetime()` or `as.Date()` |\n",
    "| Category           | `category`             | `factor`             | Useful for grouping and modeling |\n",
    "| Missing Values     | `NaN` (`numpy`)        | `NA`                 | Use `pd.isna()` or `is.na()` |\n",
    "| Complex Numbers    | `complex`              | `complex`            | Rare in typical EDA workflows |\n",
    "| List               | `list`                 | `list`               | R lists allow mixed data types |\n",
    "| Dictionary         | `dict`                 | `named list`         | R lists with names can mimic Python dictionaries |\n",
    "| Tuple              | `tuple`                | `c()`, `list()`      | No direct equivalent; use vectors or lists in R |\n",
    "\n",
    "---\n",
    "\n",
    "## Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590d95f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the standardized dataset\n",
    "df = pd.read_csv(\"data/iris.csv\")\n",
    "\n",
    "# View column names\n",
    "print(\"Column names:\", df.columns.tolist())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Optional: Use .info() for a more detailed summary\n",
    "print(\"\\nStructure info:\")\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b560aa",
   "metadata": {},
   "source": [
    "## R Code\n",
    "```{r}\n",
    "library(readr)\n",
    "\n",
    "# Load the standardized dataset\n",
    "df <- read_csv(\"data/iris.csv\")\n",
    "\n",
    "# View column names\n",
    "names(df)\n",
    "\n",
    "# Check data types (structure)\n",
    "str(df)\n",
    "\n",
    "# Optionally print class of each variable\n",
    "sapply(df, class)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "> âœ… Once you're familiar with variable types, you can decide how to clean, filter, or transform your data â€” and which variables are ready for plotting or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1aad12",
   "metadata": {},
   "source": [
    "# How do you check for missing values in Python and R?\n",
    "\n",
    "## Explanation\n",
    "\n",
    "Before performing any analysis or modeling, it's important to check for **missing values**. These can cause errors, affect summary statistics, or bias machine learning models if not handled properly.\n",
    "\n",
    "In both Python and R, missing values are represented differently:\n",
    "\n",
    "- In **Python**, missing values typically appear as `NaN` (Not a Number)\n",
    "- In **R**, they are represented as `NA`\n",
    "\n",
    "Weâ€™ll use built-in functions to:\n",
    "\n",
    "- Detect if any values are missing\n",
    "- Count missing values by column\n",
    "- (Optionally) summarize total missing values across the dataset\n",
    "\n",
    "---\n",
    "\n",
    "## Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a1f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the standardized dataset\n",
    "df = pd.read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Check if there are any missing values\n",
    "print(\"Any missing values?\", df.isnull().values.any())\n",
    "\n",
    "# Count missing values by column\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Optional: total number of missing entries\n",
    "print(\"\\nTotal missing values:\", df.isnull().sum().sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40beba54",
   "metadata": {},
   "source": [
    "## R Code\n",
    "```{r}\n",
    "library(readr)\n",
    "\n",
    "# Load the standardized dataset\n",
    "df <- read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Check if any missing values exist\n",
    "any(is.na(df))\n",
    "\n",
    "# Count missing values per column\n",
    "colSums(is.na(df))\n",
    "\n",
    "# Optional: total number of missing values\n",
    "sum(is.na(df))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "> âœ… Once you've identified missing values, the next step is to **decide how to handle them** â€” such as removing, imputing, or flagging them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64630f38",
   "metadata": {},
   "source": [
    "# How do you get summary statistics for numeric variables in Python and R?\n",
    "\n",
    "## Explanation\n",
    "\n",
    "Summary statistics provide a quick overview of your numeric data. They help you understand:\n",
    "\n",
    "- Central tendency (mean, median)\n",
    "- Spread (min, max, standard deviation, quartiles)\n",
    "- Distribution shape and potential outliers\n",
    "\n",
    "Both **Python** and **R** offer built-in functions to calculate summary statistics for each column in a dataset. These are essential when assessing data quality and preparing for visualization or modeling.\n",
    "\n",
    "---\n",
    "\n",
    "## Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e80744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Get summary statistics for all numeric columns\n",
    "summary = df.describe()\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db0e60a",
   "metadata": {},
   "source": [
    "> ðŸ’¡ `df.describe()` returns count, mean, std, min, 25%, 50% (median), 75%, and max for each numeric column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b5a83",
   "metadata": {},
   "source": [
    "## R Code\n",
    "```{r}\n",
    "library(readr)\n",
    "\n",
    "# Load the dataset\n",
    "df <- read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Get summary statistics\n",
    "summary(df)\n",
    "```\n",
    "\n",
    "> ðŸ’¡ `summary()` in R returns min, 1st quartile, median, mean, 3rd quartile, and max.\n",
    "\n",
    "---\n",
    "\n",
    "> âœ… These summaries give you a solid first look at the data distribution and can guide further steps like filtering, normalization, or visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ddaf8",
   "metadata": {},
   "source": [
    "# How do you filter rows based on a condition in Python and R?\n",
    "\n",
    "## Explanation\n",
    "\n",
    "Filtering is one of the most important skills in data wrangling. It allows you to isolate subsets of data that meet certain conditions â€” for example:\n",
    "\n",
    "- Observations above or below a threshold\n",
    "- Specific categories (e.g., only one species)\n",
    "- Logical combinations (e.g., long petals *and* wide sepals)\n",
    "\n",
    "In both Python and R, filtering uses logical expressions that evaluate to `True` or `False` for each row.\n",
    "\n",
    "In this example, weâ€™ll filter rows where:\n",
    "\n",
    "> `sepal_length > 5.0`\n",
    "\n",
    "---\n",
    "\n",
    "## Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ef08b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Filter rows where sepal_length > 5.0\n",
    "filtered_df = df[df[\"sepal_length\"] > 5.0]\n",
    "\n",
    "# View result\n",
    "print(filtered_df.head())\n",
    "\n",
    "# Confirm number of rows\n",
    "print(\"Filtered rows:\", filtered_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb9d1c",
   "metadata": {},
   "source": [
    "## R Code\n",
    "```{r}\n",
    "library(readr)\n",
    "library(dplyr)\n",
    "\n",
    "# Load dataset\n",
    "df <- read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Filter rows where sepal_length > 5.0\n",
    "filtered_df <- df %>%\n",
    "  filter(sepal_length > 5.0)\n",
    "\n",
    "# View result\n",
    "head(filtered_df)\n",
    "\n",
    "# Confirm number of rows\n",
    "nrow(filtered_df)\n",
    "```\n",
    "\n",
    "\n",
    "> âœ… Filtering is the gateway to conditional analysis â€” you can combine multiple conditions and pipe the result into visualizations or summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42219820",
   "metadata": {},
   "source": [
    "# How do you sort rows based on a variable in Python and R?\n",
    "\n",
    "## Explanation\n",
    "\n",
    "Sorting lets you **organize data** by numeric values, text, or any other column. Itâ€™s useful for:\n",
    "\n",
    "- Finding top/bottom performers\n",
    "- Preparing plots or tables\n",
    "- Detecting outliers and patterns\n",
    "\n",
    "In this example, weâ€™ll sort the Iris dataset by `petal_length` in **descending order** â€” meaning longest petals appear first.\n",
    "\n",
    "---\n",
    "\n",
    "## Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9684ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Sort by petal_length (descending)\n",
    "sorted_df = df.sort_values(by=\"petal_length\", ascending=False)\n",
    "\n",
    "# View result\n",
    "print(sorted_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a06023",
   "metadata": {},
   "source": [
    "## R Code\n",
    "```{r}\n",
    "library(readr)\n",
    "library(dplyr)\n",
    "\n",
    "# Load dataset\n",
    "df <- read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Sort by petal_length (descending)\n",
    "sorted_df <- df %>%\n",
    "  arrange(desc(petal_length))\n",
    "\n",
    "# View result\n",
    "head(sorted_df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "> âœ… Sorting helps highlight extremes and trends. You can sort by multiple columns or chain it with filtering for advanced workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7882278",
   "metadata": {},
   "source": [
    "# How do you create a new variable in Python and R?\n",
    "\n",
    "## Explanation\n",
    "\n",
    "Creating new variables â€” also called **feature engineering** â€” allows you to extract more insight from your data. A new variable can be based on:\n",
    "\n",
    "- Arithmetic between columns  \n",
    "- Logical comparisons  \n",
    "- Conditional rules\n",
    "\n",
    "In this example, weâ€™ll create a new column called `petal_ratio`, calculated as:\n",
    "\n",
    "> `petal_length / petal_width`\n",
    "\n",
    "This ratio can help distinguish species based on shape.\n",
    "\n",
    "---\n",
    "\n",
    "## Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7294a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Create a new column\n",
    "df[\"petal_ratio\"] = df[\"petal_length\"] / df[\"petal_width\"]\n",
    "\n",
    "# Preview result\n",
    "print(df[[\"petal_length\", \"petal_width\", \"petal_ratio\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b0a907",
   "metadata": {},
   "source": [
    "## R Code\n",
    "```{r}\n",
    "library(readr)\n",
    "library(dplyr)\n",
    "\n",
    "# Load dataset\n",
    "df <- read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Create a new column\n",
    "df <- df %>%\n",
    "  mutate(petal_ratio = petal_length / petal_width)\n",
    "\n",
    "# Preview result\n",
    "df %>%\n",
    "  select(petal_length, petal_width, petal_ratio) %>%\n",
    "  head()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "> âœ… Creating new variables gives you more ways to explore and model your data â€” itâ€™s a key step in both EDA and machine learning pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27c5e31",
   "metadata": {},
   "source": [
    "# How do you detect and remove duplicate rows in Python and R?\n",
    "\n",
    "## Explanation\n",
    "\n",
    "Duplicate rows can arise from data entry errors, merging datasets, or exporting data multiple times. Identifying and removing them is an important step in **data cleaning** to ensure that your analysis isnâ€™t biased or inflated.\n",
    "\n",
    "In both Python and R, we can:\n",
    "\n",
    "- Detect duplicates  \n",
    "- Count them  \n",
    "- Drop them if needed\n",
    "\n",
    "---\n",
    "\n",
    "## Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50adcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicates = df.duplicated()\n",
    "print(\"Any duplicates?\", duplicates.any())\n",
    "\n",
    "# Count duplicate rows\n",
    "print(\"Number of duplicates:\", duplicates.sum())\n",
    "\n",
    "# Remove duplicates\n",
    "df_cleaned = df.drop_duplicates()\n",
    "\n",
    "# Confirm removal\n",
    "print(\"New shape:\", df_cleaned.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a7d726",
   "metadata": {},
   "source": [
    "## R Code\n",
    "```{r}\n",
    "library(readr)\n",
    "library(dplyr)\n",
    "\n",
    "# Load dataset\n",
    "df <- read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicates <- duplicated(df)\n",
    "cat(\"Any duplicates?\", any(duplicates), \"\\n\")\n",
    "cat(\"Number of duplicates:\", sum(duplicates), \"\\n\")\n",
    "\n",
    "# Remove duplicates\n",
    "df_cleaned <- df %>%\n",
    "  distinct()\n",
    "\n",
    "# Confirm new size\n",
    "cat(\"New number of rows:\", nrow(df_cleaned), \"\\n\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "> âœ… Cleaning duplicates ensures your results reflect **true observations** and not duplicated data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2ee202",
   "metadata": {},
   "source": [
    "# How do you export a cleaned dataset in Python and R?\n",
    "\n",
    "## Explanation\n",
    "\n",
    "After cleaning and transforming your data â€” renaming columns, removing duplicates, creating new variables â€” itâ€™s good practice to **save the final version**.\n",
    "\n",
    "This ensures:\n",
    "\n",
    "- You donâ€™t have to redo your work  \n",
    "- Others can use the clean data  \n",
    "- You can start fresh from a reliable version for future steps (like visualization or modeling)\n",
    "\n",
    "In this example, weâ€™ll export our cleaned Iris dataset to a CSV file called `iris_cleaned.csv` in the `data/` folder.\n",
    "\n",
    "---\n",
    "\n",
    "## Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f7e47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and optionally clean dataset\n",
    "df = pd.read_csv(\"data/iris.csv\")\n",
    "df_cleaned = df.drop_duplicates()\n",
    "\n",
    "# Export to new CSV file\n",
    "df_cleaned.to_csv(\"data/iris_cleaned.csv\", index=False)\n",
    "print(\"Cleaned dataset saved as data/iris_cleaned.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813898b2",
   "metadata": {},
   "source": [
    "## R Code\n",
    "```{r}\n",
    "library(readr)\n",
    "library(dplyr)\n",
    "\n",
    "# Load and optionally clean dataset\n",
    "df <- read_csv(\"data/iris.csv\")\n",
    "df_cleaned <- df %>%\n",
    "  distinct()\n",
    "\n",
    "# Export to new CSV file\n",
    "write_csv(df_cleaned, \"data/iris_cleaned.csv\")\n",
    "cat(\"Cleaned dataset saved as data/iris_cleaned.csv\\n\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "> âœ… With your cleaned dataset saved, you're now ready to begin **visualizing**, **modeling**, or **sharing** your data â€” with confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecb9ab9",
   "metadata": {},
   "source": [
    "# How do you convert variable types in Python and R?\n",
    "\n",
    "## Explanation\n",
    "\n",
    "Sometimes your data has columns in the wrong type â€” for example, a numeric column stored as text, or a categorical variable treated as a string. This can affect grouping, plotting, or modeling.\n",
    "\n",
    "In this example, weâ€™ll convert:\n",
    "\n",
    "- The species column to a categorical variable\n",
    "- A numeric column to string (for labeling)\n",
    "\n",
    "---\n",
    "\n",
    "## Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205aafb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Convert species to categorical\n",
    "df[\"species\"] = df[\"species\"].astype(\"category\")\n",
    "\n",
    "# Convert sepal_length to string (optional use case)\n",
    "df[\"sepal_length_str\"] = df[\"sepal_length\"].astype(str)\n",
    "\n",
    "# Confirm types\n",
    "print(df.dtypes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5904aa18",
   "metadata": {},
   "source": [
    "## R Code\n",
    "\n",
    "```{r}\n",
    "library(readr)\n",
    "library(dplyr)\n",
    "\n",
    "# Load dataset\n",
    "df <- read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Convert species to factor\n",
    "df <- df %>%\n",
    "  mutate(species = as.factor(species))\n",
    "\n",
    "# Convert sepal_length to character\n",
    "df <- df %>%\n",
    "  mutate(sepal_length_str = as.character(sepal_length))\n",
    "\n",
    "# Confirm structure\n",
    "str(df)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f245db",
   "metadata": {},
   "source": [
    "> âœ… Converting variable types ensures that each column behaves correctly in your analysis or visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4009efaf",
   "metadata": {},
   "source": [
    "# How do you group and summarize data in Python and R?\n",
    "\n",
    "## Explanation\n",
    "\n",
    "Grouping data lets you compare subsets â€” like computing the average petal length for each species. This is a powerful technique for understanding patterns and trends.\n",
    "\n",
    "---\n",
    "\n",
    "## Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f46ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Group by species and compute average petal length\n",
    "grouped = df.groupby(\"species\")[\"petal_length\"].mean().reset_index()\n",
    "\n",
    "print(grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bcfa38",
   "metadata": {},
   "source": [
    "## R Code\n",
    "\n",
    "```{r}\n",
    "library(readr)\n",
    "library(dplyr)\n",
    "\n",
    "# Load dataset\n",
    "df <- read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Group by species and summarize\n",
    "df_summary <- df %>%\n",
    "  group_by(species) %>%\n",
    "  summarise(avg_petal_length = mean(petal_length, na.rm = TRUE))\n",
    "\n",
    "df_summary\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4e305e",
   "metadata": {},
   "source": [
    "> âœ… Grouping and summarizing help uncover relationships across categories in your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679c67e",
   "metadata": {},
   "source": [
    "# How do you drop or reorder columns in Python and R?\n",
    "\n",
    "## Explanation\n",
    "\n",
    "Sometimes you want to exclude a column or rearrange the order of columns for reporting or modeling. This improves readability and usability.\n",
    "\n",
    "---\n",
    "\n",
    "## Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0e2db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Drop petal_width column\n",
    "df_dropped = df.drop(columns=[\"petal_width\"])\n",
    "\n",
    "# Reorder columns\n",
    "cols = [\"species\", \"sepal_length\", \"sepal_width\", \"petal_length\"]\n",
    "df_reordered = df[cols]\n",
    "\n",
    "print(df_reordered.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deccc445",
   "metadata": {},
   "source": [
    "## R Code\n",
    "\n",
    "```{r}\n",
    "library(readr)\n",
    "library(dplyr)\n",
    "\n",
    "# Load dataset\n",
    "df <- read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Drop petal_width column\n",
    "df_dropped <- df %>%\n",
    "  select(-petal_width)\n",
    "\n",
    "# Reorder columns\n",
    "df_reordered <- df %>%\n",
    "  select(species, sepal_length, sepal_width, petal_length)\n",
    "\n",
    "head(df_reordered)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613ae1b9",
   "metadata": {},
   "source": [
    "> âœ… Dropping and reordering columns gives you control over which features to keep and how to present them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85be1084",
   "metadata": {},
   "source": [
    "# How do you subset specific columns in Python and R?\n",
    "\n",
    "## Explanation\n",
    "\n",
    "You may want to work with only a few columns at a time â€” for visualization, inspection, or modeling. This helps reduce clutter and focus on key variables.\n",
    "\n",
    "---\n",
    "\n",
    "## Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5a16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Select specific columns\n",
    "subset = df[[\"sepal_length\", \"sepal_width\", \"species\"]]\n",
    "\n",
    "print(subset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a14381a",
   "metadata": {},
   "source": [
    "## R Code\n",
    "\n",
    "```{r}\n",
    "library(readr)\n",
    "library(dplyr)\n",
    "\n",
    "# Load dataset\n",
    "df <- read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Select specific columns\n",
    "subset <- df %>%\n",
    "  select(sepal_length, sepal_width, species)\n",
    "\n",
    "head(subset)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef30db70",
   "metadata": {},
   "source": [
    "> âœ… Subsetting lets you focus your analysis on the most relevant columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab203ba1",
   "metadata": {},
   "source": [
    "# How do you sample rows randomly in Python and R?\n",
    "\n",
    "## Explanation\n",
    "\n",
    "Sampling is useful when working with large datasets, doing quick checks, or creating training/test splits. You can randomly select a few rows for inspection.\n",
    "\n",
    "---\n",
    "\n",
    "## Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a83a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Random sample of 5 rows\n",
    "sampled = df.sample(n=5, random_state=42)\n",
    "\n",
    "print(sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db25aef2",
   "metadata": {},
   "source": [
    "## R Code\n",
    "\n",
    "```{r}\n",
    "library(readr)\n",
    "library(dplyr)\n",
    "\n",
    "# Load dataset\n",
    "df <- read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Random sample of 5 rows\n",
    "set.seed(42)\n",
    "sampled <- df %>%\n",
    "  sample_n(5)\n",
    "\n",
    "sampled\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcf717f",
   "metadata": {},
   "source": [
    "> âœ… Sampling allows you to explore or test your data without loading the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de2ee37",
   "metadata": {},
   "source": [
    "# How do you take a random sample from a large dataset in Python and R?\n",
    "\n",
    "## Explanation\n",
    "\n",
    "Sampling is useful when working with large datasets, doing quick inspections, or preparing training/test splits. It allows you to explore a manageable portion of the data without loading everything.\n",
    "\n",
    "In this example, weâ€™ll take a **random sample of 500 rows** from the `diamonds` dataset â€” which is considerably larger and more varied than the `iris` dataset we've used so far.\n",
    "\n",
    "### ðŸŽ¨ Why Switch to the Diamonds Dataset? {-}\n",
    "\n",
    "The **Iris dataset** is a great starting point for learning structure, filtering, and variable creation â€” but itâ€™s small and limited in variety.\n",
    "\n",
    "The **Diamonds dataset** is much larger and richer, with:\n",
    "\n",
    "- Categorical variables like `cut`, `color`, and `clarity`  \n",
    "- Continuous variables like `price`, `carat`, and dimensions  \n",
    "- Greater potential for beautiful and insightful visualizations\n",
    "\n",
    "Weâ€™ll continue using **Iris** for clarity and comparison, but starting in the **Visualization Layer**, youâ€™ll also work with a **sampled Diamonds dataset** to practice chart types like:\n",
    "\n",
    "- Boxplots grouped by cut or clarity  \n",
    "- Scatter plots of carat vs. price  \n",
    "- Histograms of distribution patterns\n",
    "\n",
    "Sampling allows us to keep things fast and responsive while still leveraging the power of a large, visual-friendly dataset.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Python Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfbe284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Load full diamonds dataset\n",
    "diamonds = sns.load_dataset(\"diamonds\")\n",
    "\n",
    "# Take a random sample of 500 rows\n",
    "sampled_diamonds = diamonds.sample(n=500, random_state=42)\n",
    "\n",
    "# Save to CSV (optional)\n",
    "sampled_diamonds.to_csv(\"data/diamonds_sample.csv\", index=False)\n",
    "\n",
    "# View first few rows\n",
    "print(sampled_diamonds.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1815bd",
   "metadata": {},
   "source": [
    "## R Code\n",
    "\n",
    "```{r}\n",
    "library(ggplot2)\n",
    "library(dplyr)\n",
    "library(readr)\n",
    "\n",
    "# Load full diamonds dataset\n",
    "data(\"diamonds\")  # loads dataset into the environment\n",
    "\n",
    "# Access the dataset\n",
    "diamonds_df <- diamonds\n",
    "\n",
    "# Take a random sample of 500 rows\n",
    "set.seed(42)\n",
    "diamonds_sample <- sample_n(diamonds_df, 500)\n",
    "\n",
    "# Save to CSV (optional)\n",
    "write_csv(diamonds_sample, \"data/diamonds_sample.csv\")\n",
    "\n",
    "# View first few rows\n",
    "head(diamonds_sample)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d89b41",
   "metadata": {},
   "source": [
    "# EDA Summary {-}\n",
    "\n",
    "Youâ€™ve successfully completed the **Exploratory Data Analysis (EDA)** layer of the CDI Learning System â€” working hands-on in both Python and R to load, inspect, clean, transform, and save your data.\n",
    "\n",
    "This foundational layer has equipped you with the confidence to move forward â€” not just knowing how to use code, but how to think like a data scientist.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§± What Youâ€™ve Accomplished {-}\n",
    "\n",
    "- âœ… Set up a clean, organized project workspace  \n",
    "- âœ… Practiced essential EDA techniques with real data  \n",
    "- âœ… Standardized and saved a reusable dataset (data/iris.csv)  \n",
    "- âœ… Built fluency in both Python and R side by side\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ˆ What Comes After EDA? {-}\n",
    "\n",
    "Now that youâ€™ve completed the **Exploratory Data Analysis (EDA)** layer, youâ€™re ready to move beyond structure and cleaning â€” and into **data storytelling**.\n",
    "\n",
    "The next stages of your journey include:\n",
    "\n",
    "- ðŸŽ¨ **Data Visualization (VIZ)** â€” turn data into clear, compelling visual insights  \n",
    "- ðŸ“ **Statistical Analysis (STATS)** â€” test hypotheses and draw valid conclusions  \n",
    "- ðŸ¤– **Machine Learning (ML)** â€” build predictive models using real-world data\n",
    "\n",
    "In these upcoming layers, youâ€™ll continue working with familiar datasets â€” and explore new ones tailored to each domain.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Continue Learning with CDI {-}\n",
    "\n",
    "Looking to go further or dive into another domain?\n",
    "\n",
    "ðŸ“š **[Explore All CDI Products â†’](https://complexdatainsights.com/explore-products/)**\n",
    "\n",
    "> âœ… With a strong analytical foundation, you're now equipped to **structure, inspect, and transform data confidently â€” preparing it for visualization, statistical analysis, and machine learning.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
