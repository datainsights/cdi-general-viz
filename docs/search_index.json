[["index.html", "General Data Science – Free Edition ", " General Data Science – Free Edition Last updated: June 20, 2025 "],["welcome-to-the-general-data-science-exploratory-data-analysis-eda-layer.html", "Welcome to the General Data Science – Exploratory Data Analysis (EDA) Layer 📘 What You’ll Learn", " Welcome to the General Data Science – Exploratory Data Analysis (EDA) Layer Welcome to Complex Data Insights (CDI) — a dynamic learning system where data science is taught through practical, question-based exploration. This guide — the General Data Science: Free Q&amp;A Edition — is your gateway into the CDI ecosystem. It’s designed for beginners ready to build a strong foundation in data analysis using both Python and R, side by side. You’ll work through real-world questions with step-by-step walkthroughs that show not just how to write code — but how to think like a data scientist. 📘 What You’ll Learn Each Q&amp;A is designed to help you learn by doing: ✅ Hands-on code examples in Python and R 📊 Meaningful datasets for practice and exploration 💡 Clear, actionable explanations that build confidence In this Exploratory Data Analysis (EDA) layer, you’ll practice essential skills like: Loading and inspecting data Cleaning and transforming datasets Creating new variables and saving results Preparing for visualization and modeling 📢 For Job Seekers &amp; Lifelong Learners “You’re not going to lose your job to an AI; you’ll lose it to someone who uses AI.” — Jensen Huang, Milken Institute Global Conference, May 6, 2025 In today’s data-driven world, strong analytical and coding skills — in Python, R, and beyond — are your ticket to career flexibility, confidence, and long-term relevance — no matter where you work or learn. ✅ Build future-ready skills that stand out ✅ Learn how to think like a data scientist — not just code like one ✅ Use AI as your ally, not your competitor Let’s get started. "],["setting-up-your-analysis-environment.html", "Setting Up Your Analysis Environment Explanation 🔹 Who This Is For Install Python Install R and RStudio 🔹 Install VSCode (Recommended Editor) Setup with venv for Python Setup with renv for R", " Setting Up Your Analysis Environment Explanation Before diving into the Q&amp;As, let’s make sure your system is ready. This section walks you through installing the essential tools needed to run all examples in this guide — using both Python and R side by side. It’s designed with beginners in mind and focuses on tools that are: ✅ Free and open-source ✅ Cross-platform (Windows, macOS, Linux) ✅ Commonly used in real-world data science workflows 🔹 Who This Is For This setup guide is ideal for: Learners new to Python, R, or data science Students and professionals starting their first analysis project Anyone looking to build a clean, reproducible workflow 💡 You don’t need to be a programmer to follow along. Every Q&amp;A is explained clearly — with real-world tasks and paired code in both Python and R. Let’s begin by installing the core tools that support this dual-language learning experience. Install Python Visit the official Python page: python.org/downloads Download Python 3.9 or later ✅ During installation, check “Add Python to PATH” To confirm it’s installed, open your terminal or command prompt and run: python --version Install R and RStudio Download and install R from CRAN Then download RStudio Desktop (free version) from posit.co Verify installation in the R console by running: version 🔹 Install VSCode (Recommended Editor) Visual Studio Code (VSCode) is a free, cross-platform code editor that works well with Python, R, Jupyter notebooks, Git, and more. After installing VSCode: Open the Extensions tab (Ctrl + Shift + X or Cmd + Shift + X) Search and install the following: ✅ Python Extension (by Microsoft) IntelliSense (autocompletion, parameter hints) Built-in debugging tools Jupyter notebook integration ✅ R Extension Detects installed R versions Enables inline execution and R terminal support Setup with venv for Python # Create virtual environment (if not yet created) python -m venv venv # Activate it (on Mac/Linux) source venv/bin/activate # Activate it (on Windows) venv\\Scripts\\activate # Install dependencies pip install -r requirements.txt Setup with renv for R # Install and initialize renv (only once) install.packages(&quot;renv&quot;) renv::init() # Restore the environment (after cloning) renv::restore() ✅ This ensures R and Python packages remain consistent across machines and contributors. "],["verifying-your-setup.html", "Verifying Your Setup", " Verifying Your Setup Try these checks to confirm everything is installed correctly: Open a .py file → Python syntax should highlight; autocompletion should work Open a .R file → R console should activate in VSCode or work in RStudio Run these commands in your terminal: python --version R --version ✅ If both respond correctly and editors behave as expected, you’re ready to start learning! "],["how-to-navigate-this-guide.html", "How to Navigate This Guide 🔁 Tips for Side-by-Side Learning ✅ What’s Next?", " How to Navigate This Guide Each section in this guide follows a clear Q&amp;A format with: Question — A real-world prompt or task Explanation — The key concept and goal Python Code — A full working solution in Python R Code — The equivalent approach in R This dual-language structure helps you compare approaches, build fluency, and expand your analytical thinking. 🔁 Tips for Side-by-Side Learning Use the same dataset (e.g., data/iris.csv) for both Python and R Run code independently in: Jupyter Notebook or VSCode (Python) RStudio or VSCode (R) Modify code to explore variations Compare outputs to understand differences in syntax and behavior ✅ What’s Next? You’re ready to begin working with real data. In the next section, you’ll: Set up a clean project directory Load your first dataset Begin with Exploratory Data Analysis (EDA) in Python and R Let’s get started. "],["how-do-you-create-a-project-directory-ready-for-analysis.html", "Q&A 1 How do you create a project directory ready for analysis? 1.1 Explanation 1.2 Bash (Terminal) 1.3 Python Code 1.4 R Code", " Q&A 1 How do you create a project directory ready for analysis? 1.1 Explanation Before working with data, it’s important to set up a clean and organized project directory. A consistent folder structure helps you manage scripts, datasets, and outputs across both Python and R — making your work easier to follow and share. In this guide, we’ll create a root directory called general-data-science with four folders: data/ – for datasets scripts/ – for code files images/ – for plots and charts library/ – for reusable functions Example Folder Structure: general-data-science/ ├── data/ ├── scripts/ ├── images/ └── library/ 1.2 Bash (Terminal) You can create the entire structure using this single command: mkdir -p general-data-science/{data,scripts,images,library} cd general-data-science 1.3 Python Code You can also create the same folder structure in Python: import os folders = [&quot;data&quot;, &quot;scripts&quot;, &quot;images&quot;, &quot;library&quot;] root = &quot;general-data-science&quot; os.makedirs(root, exist_ok=True) for folder in folders: os.makedirs(os.path.join(root, folder), exist_ok=True) print(f&quot;Created &#39;{root}&#39; project folder with subdirectories.&quot;) 1.4 R Code Here’s how to do it in R: folders &lt;- c(&quot;data&quot;, &quot;scripts&quot;, &quot;images&quot;, &quot;library&quot;) root &lt;- &quot;general-data-science&quot; if (!dir.exists(root)) dir.create(root) for (folder in folders) { dir.create(file.path(root, folder), showWarnings = FALSE) } cat(&quot;Created&quot;, root, &quot;project folder with subdirectories.\\n&quot;) ✅ A clean project directory helps you stay organized, reuse code, and avoid errors — it’s the first step toward reproducible, professional data science. "],["how-do-you-install-basic-data-science-tools-and-libraries-for-python-and-r.html", "Q&A 2 How do you install basic data science tools and libraries for Python and R? 2.1 Explanation 2.2 Python Tools 2.3 R Tools", " Q&A 2 How do you install basic data science tools and libraries for Python and R? 2.1 Explanation Before you can analyze data in Python or R, you need to install essential libraries. These libraries provide tools for data manipulation, visualization, statistical analysis, and machine learning — the four core layers in the CDI learning system. Installing them ensures you’re ready to explore datasets and build reproducible workflows. 2.2 Python Tools In your terminal or command prompt, run: pip install pandas numpy matplotlib seaborn scikit-learn scipy Then, import and check versions to confirm installation: import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import sklearn import scipy from scipy import stats # still useful to import for use print(&quot;pandas:&quot;, pd.__version__) print(&quot;numpy:&quot;, np.__version__) print(&quot;matplotlib:&quot;, plt.matplotlib.__version__) print(&quot;seaborn:&quot;, sns.__version__) print(&quot;scikit-learn:&quot;, sklearn.__version__) print(&quot;scipy:&quot;, scipy.__version__) pandas: 2.2.3 numpy: 2.1.3 matplotlib: 3.10.0 seaborn: 0.13.2 scikit-learn: 1.6.0 scipy: 1.14.1 Installed Python libraries by layer: 🧹 EDA: pandas – Tabular data structures and data cleaning tools numpy – Efficient array operations for numerical computing 📊 Visualization: matplotlib – Customizable static and interactive plots seaborn – Statistical data visualizations built on matplotlib 📐 Statistical Analysis: scipy.stats – Tools for distributions, t-tests, ANOVA, correlation, and more 🤖 Machine Learning: scikit-learn – Algorithms and utilities for classification, regression, clustering, model evaluation 2.3 R Tools # ----------------------------- # 📊 EDA (Exploratory Data Analysis) # ----------------------------- if (!require(tidyverse)) install.packages(&quot;tidyverse&quot;) library(tidyverse) # ----------------------------- # 📈 Visualization # ----------------------------- if (!require(GGally)) install.packages(&quot;GGally&quot;) library(GGally) # ----------------------------- # 📐 Statistical Analysis (STATS) # ----------------------------- if (!require(broom)) install.packages(&quot;broom&quot;) library(broom) if (!require(car)) install.packages(&quot;car&quot;) library(car) if (!require(emmeans)) install.packages(&quot;emmeans&quot;) library(emmeans) # ----------------------------- # 🤖 Machine Learning # ----------------------------- if (!require(caret)) install.packages(&quot;caret&quot;) library(caret) Installed R packages by layer: 🧹 EDA: tidyverse – A collection of packages for tidy data workflows: dplyr (data manipulation) readr (reading CSV and text files) tidyr (reshaping data) tibble (modern data frames) stringr (string operations) forcats (working with factors) ggplot2 (visualization) purrr (functional programming) 📊 Visualization: ggplot2 – Grammar of graphics for elegant visualizations (included in tidyverse) GGally – Enhances ggplot2 with matrix plots, correlation plots, etc. 📐 Statistical Analysis: broom – Converts model outputs into tidy data frames car – Tools for regression diagnostics, ANOVA, linear models emmeans – Estimated marginal means for post-hoc testing and comparisons 🤖 Machine Learning: caret – A unified framework for training, tuning, and comparing models across many algorithms ✅ Once these tools are installed, you’ll be ready to acquire datasets and begin your analysis. "],["what-are-common-sources-of-datasets-for-python-and-r.html", "Q&A 3 What are common sources of datasets for Python and R? 3.1 Explanation 3.2 Python Package-Based Datasets 3.3 R Package-Based Datasets 3.4 Online Public Data Sources", " Q&A 3 What are common sources of datasets for Python and R? 3.1 Explanation Before working with data, it’s important to know where data comes from. In both Python and R, you can use: Public datasets from libraries or platforms Downloaded datasets from repositories Real-world data from research, surveys, APIs, or government sources These sources help you practice data skills using real, structured information. Common sources include: Built-in datasets: Python: seaborn, sklearn.datasets, statsmodels, pydataset R: datasets package, MASS, ggplot2, palmerpenguins Online repositories: UCI Machine Learning Repository Kaggle Datasets data.gov WHO, UN, World Bank Research &amp; Surveys: CSV/Excel/JSON files published with academic papers or institutions Survey data from organizations (e.g., Pew Research, Eurostat) APIs and live feeds: Weather, financial markets, genomics, social media (e.g., Twitter API) Local files: Saved from tools like Excel, Google Sheets, SPSS, or exported from databases 3.2 Python Package-Based Datasets Seaborn: import seaborn as sns iris = sns.load_dataset(&quot;iris&quot;) print(iris[:5]) sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa Scikit-learn: from sklearn import datasets irisml = datasets.load_iris() print(irisml.data[:5]) [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2] [4.7 3.2 1.3 0.2] [4.6 3.1 1.5 0.2] [5. 3.6 1.4 0.2]] Statsmodels: import statsmodels.api as sm df = sm.datasets.get_rdataset(“Guerry”, “HistData”).data 3.3 R Package-Based Datasets datasets package: iris &lt;- datasets::iris head(iris) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 1 5.1 3.5 1.4 0.2 setosa 2 4.9 3.0 1.4 0.2 setosa 3 4.7 3.2 1.3 0.2 setosa 4 4.6 3.1 1.5 0.2 setosa 5 5.0 3.6 1.4 0.2 setosa 6 5.4 3.9 1.7 0.4 setosa ggplot2: data(&quot;diamonds&quot;, package = &quot;ggplot2&quot;) head(diamonds) carat cut color clarity depth table price x y z 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4 0.29 Premium I VS2 62.4 58 334 4.20 4.23 2.63 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 palmerpenguins (if installed): library(palmerpenguins) head(penguins) # A tibble: 6 × 8 species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; 1 Adelie Torgersen 39.1 18.7 181 3750 2 Adelie Torgersen 39.5 17.4 186 3800 3 Adelie Torgersen 40.3 18 195 3250 4 Adelie Torgersen NA NA NA NA 5 Adelie Torgersen 36.7 19.3 193 3450 6 Adelie Torgersen 39.3 20.6 190 3650 # ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; 3.4 Online Public Data Sources Source Link UCI Machine Learning Repo https://archive.ics.uci.edu/ml/ Kaggle Datasets https://www.kaggle.com/datasets data.gov (US Government) https://www.data.gov Awesome Public Datasets https://github.com/awesomedata/awesome-public-datasets World Bank Open Data https://data.worldbank.org/ 💡 Tip: Always save downloaded datasets in your data/ folder and reference them using relative paths like data/filename.csv. ✅ Now that you know where to find data, let’s learn how to save, load and preview it in your Python or R environment. "],["how-do-you-save-a-dataset-in-python-and-r.html", "Q&A 4 How do you save a dataset in Python and R? 4.1 Explanation 4.2 Python Code 4.3 R Code", " Q&A 4 How do you save a dataset in Python and R? 4.1 Explanation Once you’ve cleaned or prepared a dataset, it’s good practice to save it in a standard format like CSV. This allows you to: Preserve your cleaned version for future use Avoid repeating preprocessing steps Share your data with others or load it in different tools In this example, we’ll use sample datasets provided by libraries in Python and R, then save them into the data/ folder using to_csv() in Python and write_csv() in R. 4.2 Python Code import pandas as pd from sklearn import datasets import seaborn as sns import os # Create data folder os.makedirs(&quot;data&quot;, exist_ok=True) # Save seaborn&#39;s iris dataset df_iris_seaborn = sns.load_dataset(&quot;iris&quot;) df_iris_seaborn.to_csv(&quot;data/iris_seaborn.csv&quot;, index=False) print(&quot;\\nSeaborn Iris\\n&quot;, df_iris_seaborn.head()) # Save sklearn iris as well (optional) iris_sklearn = datasets.load_iris(as_frame=True).frame iris_sklearn.to_csv(&quot;data/iris_sklearn.csv&quot;, index=False) print(&quot;\\nSklearn Iris\\n&quot;, iris_sklearn.head()) print(&quot;Datasets saved successfully.&quot;) Seaborn Iris sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa Sklearn Iris sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) \\ 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 target 0 0 1 0 2 0 3 0 4 0 Datasets saved successfully. 4.3 R Code # Load necessary libraries library(readr) library(datasets) # Create &#39;data/&#39; directory if it doesn&#39;t exist if (!dir.exists(&quot;data&quot;)) dir.create(&quot;data&quot;) # Save the built-in iris dataset write_csv(iris, &quot;data/iris_rbase.csv&quot;) cat(&quot;Datasets saved successfully.\\n&quot;) Datasets saved successfully. ✅ After saving your cleaned or example dataset, you can now load it for further analysis or visualization in future sessions. "],["how-do-you-load-a-pre-cleaned-dataset-in-python-and-r.html", "Q&A 5 How do you load a pre-cleaned dataset in Python and R? 5.1 Explanation 5.2 Python Code 5.3 R Code", " Q&A 5 How do you load a pre-cleaned dataset in Python and R? 5.1 Explanation Loading a dataset is one of the first steps in any data analysis project — especially when you’re working from a previously saved, cleaned version. In this guide, we assume that the dataset *.csv has been saved in your data/ folder. We’ll now load it using: Python: via the pandas library and its read_csv() function R: using the readr package and its read_csv() function Using consistent file paths (like data/*.csv) ensures reproducibility across environments. 5.2 Python Code import pandas as pd # Load the pre-cleaned iris dataset df = pd.read_csv(&quot;data/iris_seaborn.csv&quot;) # Preview the data print(df.head()) # Confirm the shape print(&quot;Rows and columns:&quot;, df.shape) sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa Rows and columns: (150, 5) 5.3 R Code library(readr) # Load the pre-cleaned iris dataset df &lt;- read_csv(&quot;data/iris_rbase.csv&quot;) # Preview the data head(df) # A tibble: 6 × 5 Sepal.Length Sepal.Width Petal.Length Petal.Width Species &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 5.1 3.5 1.4 0.2 setosa 2 4.9 3 1.4 0.2 setosa 3 4.7 3.2 1.3 0.2 setosa 4 4.6 3.1 1.5 0.2 setosa 5 5 3.6 1.4 0.2 setosa 6 5.4 3.9 1.7 0.4 setosa # Dimensions cat(&quot;Rows and columns:&quot;, dim(df), &quot;\\n&quot;) Rows and columns: 150 5 ✅ Once loaded, you’re ready to continue with data wrangling, visualization, or modeling — all based on your clean, reusable dataset. "],["how-do-you-rename-column-names-in-python-and-r.html", "Q&A 6 How do you rename column names in Python and R? 6.1 Explanation 6.2 Python Code 6.3 R Code", " Q&A 6 How do you rename column names in Python and R? 6.1 Explanation When working with the same dataset in both Python and R, you may encounter slight differences in column names — such as capitalization or spacing. To avoid confusion and ensure consistency across your analysis, it’s best to standardize the column names. In this guide, we’ll rename the columns to lowercase with underscores: sepal_length sepal_width petal_length petal_width species After renaming, we’ll save the final, standardized dataset as data/iris.csv, which will be used consistently throughout the rest of the guide. 6.2 Python Code import pandas as pd # Using seaborn version of the iris dataset df1 = pd.read_csv(&quot;data/iris_seaborn.csv&quot;) print(&quot;Original column names from seaborn version:&quot;, df1.columns.tolist()) # Rename columns df1.columns = [&quot;sepal_length&quot;, &quot;sepal_width&quot;, &quot;petal_length&quot;, &quot;petal_width&quot;, &quot;species&quot;] # Save standardized version df1.to_csv(&quot;data/iris.csv&quot;, index=False) print(&quot;Saved standardized dataset from seaborn version as &#39;data/iris.csv&#39;&quot;) Original column names from seaborn version: [&#39;sepal_length&#39;, &#39;sepal_width&#39;, &#39;petal_length&#39;, &#39;petal_width&#39;, &#39;species&#39;] Saved standardized dataset from seaborn version as &#39;data/iris.csv&#39; 6.3 R Code library(readr) library(dplyr) # Option 1: If your dataset already has lowercase column names (e.g., from iris_rbase.csv) df &lt;- read_csv(&quot;data/iris_rbase.csv&quot;) # Set standardized column names directly colnames(df) &lt;- c(&quot;sepal_length&quot;, &quot;sepal_width&quot;, &quot;petal_length&quot;, &quot;petal_width&quot;, &quot;species&quot;) # Save standardized dataset write_csv(df, &quot;data/iris.csv&quot;) cat(&quot;Saved standardized dataset from iris_rbase.csv as data/iris.csv\\n&quot;) Saved standardized dataset from iris_rbase.csv as data/iris.csv # Option 2: If your dataset has capitalized column names (e.g., from iris_rbase.csv) df &lt;- read_csv(&quot;data/iris_rbase.csv&quot;) # Rename columns using dplyr for consistency df &lt;- df %&gt;% rename( sepal_length = Sepal.Length, sepal_width = Sepal.Width, petal_length = Petal.Length, petal_width = Petal.Width, species = Species ) # Save standardized dataset write_csv(df, &quot;data/iris.csv&quot;) cat(&quot;Saved standardized dataset from iris_rbase.csv as data/iris.csv\\n&quot;) Saved standardized dataset from iris_rbase.csv as data/iris.csv ✅ From this point forward, we’ll use data/iris.csv as the unified, clean dataset for all Python and R examples in the guide. "],["how-do-you-examine-the-structure-and-types-of-variables-in-python-and-r.html", "Q&A 7 How do you examine the structure and types of variables in Python and R? 7.1 Explanation 7.2 Python Code 7.3 R Code", " Q&A 7 How do you examine the structure and types of variables in Python and R? 7.1 Explanation Understanding the structure of your dataset — including data types — is a key step in exploratory data analysis. It helps you: Know what transformations are needed Identify categorical vs. numerical variables Prepare your data for modeling or visualization Each column in your dataset has a specific data type. These types influence how operations behave, how memory is allocated, and how functions treat your data. ✅ Common Data Types in Python and R Concept Python (pandas) R (base) Notes Integer int integer Use astype(int) or as.integer() Decimal Number float numeric, double numeric in R defaults to double Text / String str, object character Use astype(str) or as.character() Logical / Boolean bool logical True/False in Python, TRUE/FALSE in R Date / Time datetime64[ns] Date, POSIXct Use pd.to_datetime() or as.Date() Category category factor Useful for grouping and modeling Missing Values NaN (numpy) NA Use pd.isna() or is.na() Complex Numbers complex complex Rare in typical EDA workflows List list list R lists allow mixed data types Dictionary dict named list R lists with names can mimic Python dictionaries Tuple tuple c(), list() No direct equivalent; use vectors or lists in R 7.2 Python Code import pandas as pd # Load the standardized dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # View column names print(&quot;Column names:&quot;, df.columns.tolist()) # Check data types print(&quot;\\nData types:&quot;) print(df.dtypes) # Optional: Use .info() for a more detailed summary print(&quot;\\nStructure info:&quot;) df.info() Column names: [&#39;sepal_length&#39;, &#39;sepal_width&#39;, &#39;petal_length&#39;, &#39;petal_width&#39;, &#39;species&#39;] Data types: sepal_length float64 sepal_width float64 petal_length float64 petal_width float64 species object dtype: object Structure info: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 150 entries, 0 to 149 Data columns (total 5 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 sepal_length 150 non-null float64 1 sepal_width 150 non-null float64 2 petal_length 150 non-null float64 3 petal_width 150 non-null float64 4 species 150 non-null object dtypes: float64(4), object(1) memory usage: 6.0+ KB 7.3 R Code library(readr) # Load the standardized dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # View column names names(df) [1] &quot;sepal_length&quot; &quot;sepal_width&quot; &quot;petal_length&quot; &quot;petal_width&quot; &quot;species&quot; # Check data types (structure) str(df) spc_tbl_ [150 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame) $ sepal_length: num [1:150] 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... $ sepal_width : num [1:150] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... $ petal_length: num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... $ petal_width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... $ species : chr [1:150] &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; ... - attr(*, &quot;spec&quot;)= .. cols( .. sepal_length = col_double(), .. sepal_width = col_double(), .. petal_length = col_double(), .. petal_width = col_double(), .. species = col_character() .. ) - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # Optionally print class of each variable sapply(df, class) sepal_length sepal_width petal_length petal_width species &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;character&quot; ✅ Once you’re familiar with variable types, you can decide how to clean, filter, or transform your data — and which variables are ready for plotting or modeling. "],["how-do-you-check-for-missing-values-in-python-and-r.html", "Q&A 8 How do you check for missing values in Python and R? 8.1 Explanation 8.2 Python Code 8.3 R Code", " Q&A 8 How do you check for missing values in Python and R? 8.1 Explanation Before performing any analysis or modeling, it’s important to check for missing values. These can cause errors, affect summary statistics, or bias machine learning models if not handled properly. In both Python and R, missing values are represented differently: In Python, missing values typically appear as NaN (Not a Number) In R, they are represented as NA We’ll use built-in functions to: Detect if any values are missing Count missing values by column (Optionally) summarize total missing values across the dataset 8.2 Python Code import pandas as pd # Load the standardized dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Check if there are any missing values print(&quot;Any missing values?&quot;, df.isnull().values.any()) # Count missing values by column print(&quot;\\nMissing values per column:&quot;) print(df.isnull().sum()) # Optional: total number of missing entries print(&quot;\\nTotal missing values:&quot;, df.isnull().sum().sum()) Any missing values? False Missing values per column: sepal_length 0 sepal_width 0 petal_length 0 petal_width 0 species 0 dtype: int64 Total missing values: 0 8.3 R Code library(readr) # Load the standardized dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Check if any missing values exist any(is.na(df)) [1] FALSE # Count missing values per column colSums(is.na(df)) sepal_length sepal_width petal_length petal_width species 0 0 0 0 0 # Optional: total number of missing values sum(is.na(df)) [1] 0 ✅ Once you’ve identified missing values, the next step is to decide how to handle them — such as removing, imputing, or flagging them. "],["how-do-you-get-summary-statistics-for-numeric-variables-in-python-and-r.html", "Q&A 9 How do you get summary statistics for numeric variables in Python and R? 9.1 Explanation 9.2 Python Code 9.3 R Code", " Q&A 9 How do you get summary statistics for numeric variables in Python and R? 9.1 Explanation Summary statistics provide a quick overview of your numeric data. They help you understand: Central tendency (mean, median) Spread (min, max, standard deviation, quartiles) Distribution shape and potential outliers Both Python and R offer built-in functions to calculate summary statistics for each column in a dataset. These are essential when assessing data quality and preparing for visualization or modeling. 9.2 Python Code import pandas as pd # Load the dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Get summary statistics for all numeric columns summary = df.describe() print(summary) sepal_length sepal_width petal_length petal_width count 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.057333 3.758000 1.199333 std 0.828066 0.435866 1.765298 0.762238 min 4.300000 2.000000 1.000000 0.100000 25% 5.100000 2.800000 1.600000 0.300000 50% 5.800000 3.000000 4.350000 1.300000 75% 6.400000 3.300000 5.100000 1.800000 max 7.900000 4.400000 6.900000 2.500000 💡 df.describe() returns count, mean, std, min, 25%, 50% (median), 75%, and max for each numeric column. 9.3 R Code library(readr) # Load the dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Get summary statistics summary(df) sepal_length sepal_width petal_length petal_width Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 Median :5.800 Median :3.000 Median :4.350 Median :1.300 Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 species Length:150 Class :character Mode :character 💡 summary() in R returns min, 1st quartile, median, mean, 3rd quartile, and max. ✅ These summaries give you a solid first look at the data distribution and can guide further steps like filtering, normalization, or visualization. "],["how-do-you-filter-rows-based-on-a-condition-in-python-and-r.html", "Q&A 10 How do you filter rows based on a condition in Python and R? 10.1 Explanation 10.2 Python Code 10.3 R Code", " Q&A 10 How do you filter rows based on a condition in Python and R? 10.1 Explanation Filtering is one of the most important skills in data wrangling. It allows you to isolate subsets of data that meet certain conditions — for example: Observations above or below a threshold Specific categories (e.g., only one species) Logical combinations (e.g., long petals and wide sepals) In both Python and R, filtering uses logical expressions that evaluate to True or False for each row. In this example, we’ll filter rows where: sepal_length &gt; 5.0 10.2 Python Code import pandas as pd # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Filter rows where sepal_length &gt; 5.0 filtered_df = df[df[&quot;sepal_length&quot;] &gt; 5.0] # View result print(filtered_df.head()) # Confirm number of rows print(&quot;Filtered rows:&quot;, filtered_df.shape[0]) sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 5 5.4 3.9 1.7 0.4 setosa 10 5.4 3.7 1.5 0.2 setosa 14 5.8 4.0 1.2 0.2 setosa 15 5.7 4.4 1.5 0.4 setosa Filtered rows: 118 10.3 R Code library(readr) library(dplyr) # Load dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Filter rows where sepal_length &gt; 5.0 filtered_df &lt;- df %&gt;% filter(sepal_length &gt; 5.0) # View result head(filtered_df) # A tibble: 6 × 5 sepal_length sepal_width petal_length petal_width species &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 5.1 3.5 1.4 0.2 setosa 2 5.4 3.9 1.7 0.4 setosa 3 5.4 3.7 1.5 0.2 setosa 4 5.8 4 1.2 0.2 setosa 5 5.7 4.4 1.5 0.4 setosa 6 5.4 3.9 1.3 0.4 setosa # Confirm number of rows nrow(filtered_df) [1] 118 ✅ Filtering is the gateway to conditional analysis — you can combine multiple conditions and pipe the result into visualizations or summaries. "],["how-do-you-sort-rows-based-on-a-variable-in-python-and-r.html", "Q&A 11 How do you sort rows based on a variable in Python and R? 11.1 Explanation 11.2 Python Code 11.3 R Code", " Q&A 11 How do you sort rows based on a variable in Python and R? 11.1 Explanation Sorting lets you organize data by numeric values, text, or any other column. It’s useful for: Finding top/bottom performers Preparing plots or tables Detecting outliers and patterns In this example, we’ll sort the Iris dataset by petal_length in descending order — meaning longest petals appear first. 11.2 Python Code import pandas as pd # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Sort by petal_length (descending) sorted_df = df.sort_values(by=&quot;petal_length&quot;, ascending=False) # View result print(sorted_df.head()) sepal_length sepal_width petal_length petal_width species 118 7.7 2.6 6.9 2.3 virginica 122 7.7 2.8 6.7 2.0 virginica 117 7.7 3.8 6.7 2.2 virginica 105 7.6 3.0 6.6 2.1 virginica 131 7.9 3.8 6.4 2.0 virginica 11.3 R Code library(readr) library(dplyr) # Load dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Sort by petal_length (descending) sorted_df &lt;- df %&gt;% arrange(desc(petal_length)) # View result head(sorted_df) # A tibble: 6 × 5 sepal_length sepal_width petal_length petal_width species &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 7.7 2.6 6.9 2.3 virginica 2 7.7 3.8 6.7 2.2 virginica 3 7.7 2.8 6.7 2 virginica 4 7.6 3 6.6 2.1 virginica 5 7.9 3.8 6.4 2 virginica 6 7.3 2.9 6.3 1.8 virginica ✅ Sorting helps highlight extremes and trends. You can sort by multiple columns or chain it with filtering for advanced workflows. "],["how-do-you-create-a-new-variable-in-python-and-r.html", "Q&A 12 How do you create a new variable in Python and R? 12.1 Explanation 12.2 Python Code 12.3 R Code", " Q&A 12 How do you create a new variable in Python and R? 12.1 Explanation Creating new variables — also called feature engineering — allows you to extract more insight from your data. A new variable can be based on: Arithmetic between columns Logical comparisons Conditional rules In this example, we’ll create a new column called petal_ratio, calculated as: petal_length / petal_width This ratio can help distinguish species based on shape. 12.2 Python Code import pandas as pd # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Create a new column df[&quot;petal_ratio&quot;] = df[&quot;petal_length&quot;] / df[&quot;petal_width&quot;] # Preview result print(df[[&quot;petal_length&quot;, &quot;petal_width&quot;, &quot;petal_ratio&quot;]].head()) petal_length petal_width petal_ratio 0 1.4 0.2 7.0 1 1.4 0.2 7.0 2 1.3 0.2 6.5 3 1.5 0.2 7.5 4 1.4 0.2 7.0 12.3 R Code library(readr) library(dplyr) # Load dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Create a new column df &lt;- df %&gt;% mutate(petal_ratio = petal_length / petal_width) # Preview result df %&gt;% select(petal_length, petal_width, petal_ratio) %&gt;% head() # A tibble: 6 × 3 petal_length petal_width petal_ratio &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1.4 0.2 7 2 1.4 0.2 7 3 1.3 0.2 6.5 4 1.5 0.2 7.5 5 1.4 0.2 7 6 1.7 0.4 4.25 ✅ Creating new variables gives you more ways to explore and model your data — it’s a key step in both EDA and machine learning pipelines. "],["how-do-you-detect-and-remove-duplicate-rows-in-python-and-r.html", "Q&A 13 How do you detect and remove duplicate rows in Python and R? 13.1 Explanation 13.2 Python Code 13.3 R Code", " Q&A 13 How do you detect and remove duplicate rows in Python and R? 13.1 Explanation Duplicate rows can arise from data entry errors, merging datasets, or exporting data multiple times. Identifying and removing them is an important step in data cleaning to ensure that your analysis isn’t biased or inflated. In both Python and R, we can: Detect duplicates Count them Drop them if needed 13.2 Python Code import pandas as pd # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Check for duplicate rows duplicates = df.duplicated() print(&quot;Any duplicates?&quot;, duplicates.any()) # Count duplicate rows print(&quot;Number of duplicates:&quot;, duplicates.sum()) # Remove duplicates df_cleaned = df.drop_duplicates() # Confirm removal print(&quot;New shape:&quot;, df_cleaned.shape) Any duplicates? True Number of duplicates: 1 New shape: (149, 5) 13.3 R Code library(readr) library(dplyr) # Load dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Check for duplicate rows duplicates &lt;- duplicated(df) cat(&quot;Any duplicates?&quot;, any(duplicates), &quot;\\n&quot;) Any duplicates? TRUE cat(&quot;Number of duplicates:&quot;, sum(duplicates), &quot;\\n&quot;) Number of duplicates: 1 # Remove duplicates df_cleaned &lt;- df %&gt;% distinct() # Confirm new size cat(&quot;New number of rows:&quot;, nrow(df_cleaned), &quot;\\n&quot;) New number of rows: 149 ✅ Cleaning duplicates ensures your results reflect true observations and not duplicated data points. "],["how-do-you-export-a-cleaned-dataset-in-python-and-r.html", "Q&A 14 How do you export a cleaned dataset in Python and R? 14.1 Explanation 14.2 Python Code 14.3 R Code", " Q&A 14 How do you export a cleaned dataset in Python and R? 14.1 Explanation After cleaning and transforming your data — renaming columns, removing duplicates, creating new variables — it’s good practice to save the final version. This ensures: You don’t have to redo your work Others can use the clean data You can start fresh from a reliable version for future steps (like visualization or modeling) In this example, we’ll export our cleaned Iris dataset to a CSV file called iris_cleaned.csv in the data/ folder. 14.2 Python Code import pandas as pd # Load and optionally clean dataset df = pd.read_csv(&quot;data/iris.csv&quot;) df_cleaned = df.drop_duplicates() # Export to new CSV file df_cleaned.to_csv(&quot;data/iris_cleaned.csv&quot;, index=False) print(&quot;Cleaned dataset saved as data/iris_cleaned.csv&quot;) Cleaned dataset saved as data/iris_cleaned.csv 14.3 R Code library(readr) library(dplyr) # Load and optionally clean dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) df_cleaned &lt;- df %&gt;% distinct() # Export to new CSV file write_csv(df_cleaned, &quot;data/iris_cleaned.csv&quot;) cat(&quot;Cleaned dataset saved as data/iris_cleaned.csv\\n&quot;) Cleaned dataset saved as data/iris_cleaned.csv ✅ With your cleaned dataset saved, you’re now ready to begin visualizing, modeling, or sharing your data — with confidence. "],["how-do-you-convert-variable-types-in-python-and-r.html", "Q&A 15 How do you convert variable types in Python and R? 15.1 Explanation 15.2 Python Code 15.3 R Code", " Q&A 15 How do you convert variable types in Python and R? 15.1 Explanation Sometimes your data has columns in the wrong type — for example, a numeric column stored as text, or a categorical variable treated as a string. This can affect grouping, plotting, or modeling. In this example, we’ll convert: The species column to a categorical variable A numeric column to string (for labeling) 15.2 Python Code import pandas as pd # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Convert species to categorical df[&quot;species&quot;] = df[&quot;species&quot;].astype(&quot;category&quot;) # Convert sepal_length to string (optional use case) df[&quot;sepal_length_str&quot;] = df[&quot;sepal_length&quot;].astype(str) # Confirm types print(df.dtypes.head()) sepal_length float64 sepal_width float64 petal_length float64 petal_width float64 species category dtype: object 15.3 R Code library(readr) library(dplyr) # Load dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Convert species to factor df &lt;- df %&gt;% mutate(species = as.factor(species)) # Convert sepal_length to character df &lt;- df %&gt;% mutate(sepal_length_str = as.character(sepal_length)) # Confirm structure str(df) tibble [150 × 6] (S3: tbl_df/tbl/data.frame) $ sepal_length : num [1:150] 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... $ sepal_width : num [1:150] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... $ petal_length : num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... $ petal_width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... $ species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... $ sepal_length_str: chr [1:150] &quot;5.1&quot; &quot;4.9&quot; &quot;4.7&quot; &quot;4.6&quot; ... ✅ Converting variable types ensures that each column behaves correctly in your analysis or visualization. "],["how-do-you-group-and-summarize-data-in-python-and-r.html", "Q&A 16 How do you group and summarize data in Python and R? 16.1 Explanation 16.2 Python Code 16.3 R Code", " Q&A 16 How do you group and summarize data in Python and R? 16.1 Explanation Grouping data lets you compare subsets — like computing the average petal length for each species. This is a powerful technique for understanding patterns and trends. 16.2 Python Code import pandas as pd # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Group by species and compute average petal length grouped = df.groupby(&quot;species&quot;)[&quot;petal_length&quot;].mean().reset_index() print(grouped) species petal_length 0 setosa 1.462 1 versicolor 4.260 2 virginica 5.552 16.3 R Code library(readr) library(dplyr) # Load dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Group by species and summarize df_summary &lt;- df %&gt;% group_by(species) %&gt;% summarise(avg_petal_length = mean(petal_length, na.rm = TRUE)) df_summary # A tibble: 3 × 2 species avg_petal_length &lt;chr&gt; &lt;dbl&gt; 1 setosa 1.46 2 versicolor 4.26 3 virginica 5.55 ✅ Grouping and summarizing help uncover relationships across categories in your data. "],["how-do-you-drop-or-reorder-columns-in-python-and-r.html", "Q&A 17 How do you drop or reorder columns in Python and R? 17.1 Explanation 17.2 Python Code 17.3 R Code", " Q&A 17 How do you drop or reorder columns in Python and R? 17.1 Explanation Sometimes you want to exclude a column or rearrange the order of columns for reporting or modeling. This improves readability and usability. 17.2 Python Code import pandas as pd # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Drop petal_width column df_dropped = df.drop(columns=[&quot;petal_width&quot;]) # Reorder columns cols = [&quot;species&quot;, &quot;sepal_length&quot;, &quot;sepal_width&quot;, &quot;petal_length&quot;] df_reordered = df[cols] print(df_reordered.head()) species sepal_length sepal_width petal_length 0 setosa 5.1 3.5 1.4 1 setosa 4.9 3.0 1.4 2 setosa 4.7 3.2 1.3 3 setosa 4.6 3.1 1.5 4 setosa 5.0 3.6 1.4 17.3 R Code library(readr) library(dplyr) # Load dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Drop petal_width column df_dropped &lt;- df %&gt;% select(-petal_width) # Reorder columns df_reordered &lt;- df %&gt;% select(species, sepal_length, sepal_width, petal_length) head(df_reordered) # A tibble: 6 × 4 species sepal_length sepal_width petal_length &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 setosa 5.1 3.5 1.4 2 setosa 4.9 3 1.4 3 setosa 4.7 3.2 1.3 4 setosa 4.6 3.1 1.5 5 setosa 5 3.6 1.4 6 setosa 5.4 3.9 1.7 ✅ Dropping and reordering columns gives you control over which features to keep and how to present them. "],["how-do-you-subset-specific-columns-in-python-and-r.html", "Q&A 18 How do you subset specific columns in Python and R? 18.1 Explanation 18.2 Python Code 18.3 R Code", " Q&A 18 How do you subset specific columns in Python and R? 18.1 Explanation You may want to work with only a few columns at a time — for visualization, inspection, or modeling. This helps reduce clutter and focus on key variables. 18.2 Python Code import pandas as pd # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Select specific columns subset = df[[&quot;sepal_length&quot;, &quot;sepal_width&quot;, &quot;species&quot;]] print(subset.head()) sepal_length sepal_width species 0 5.1 3.5 setosa 1 4.9 3.0 setosa 2 4.7 3.2 setosa 3 4.6 3.1 setosa 4 5.0 3.6 setosa 18.3 R Code library(readr) library(dplyr) # Load dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Select specific columns subset &lt;- df %&gt;% select(sepal_length, sepal_width, species) head(subset) # A tibble: 6 × 3 sepal_length sepal_width species &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 5.1 3.5 setosa 2 4.9 3 setosa 3 4.7 3.2 setosa 4 4.6 3.1 setosa 5 5 3.6 setosa 6 5.4 3.9 setosa ✅ Subsetting lets you focus your analysis on the most relevant columns. "],["how-do-you-sample-rows-randomly-in-python-and-r.html", "Q&A 19 How do you sample rows randomly in Python and R? 19.1 Explanation 19.2 Python Code 19.3 R Code", " Q&A 19 How do you sample rows randomly in Python and R? 19.1 Explanation Sampling is useful when working with large datasets, doing quick checks, or creating training/test splits. You can randomly select a few rows for inspection. 19.2 Python Code import pandas as pd # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Random sample of 5 rows sampled = df.sample(n=5, random_state=42) print(sampled) sepal_length sepal_width petal_length petal_width species 73 6.1 2.8 4.7 1.2 versicolor 18 5.7 3.8 1.7 0.3 setosa 118 7.7 2.6 6.9 2.3 virginica 78 6.0 2.9 4.5 1.5 versicolor 76 6.8 2.8 4.8 1.4 versicolor 19.3 R Code library(readr) library(dplyr) # Load dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Random sample of 5 rows set.seed(42) sampled &lt;- df %&gt;% sample_n(5) sampled # A tibble: 5 × 5 sepal_length sepal_width petal_length petal_width species &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 5.3 3.7 1.5 0.2 setosa 2 5.6 2.9 3.6 1.3 versicolor 3 6.1 2.8 4.7 1.2 versicolor 4 6.7 3 5.2 2.3 virginica 5 5.6 2.8 4.9 2 virginica ✅ Sampling allows you to explore or test your data without loading the entire dataset. "],["how-do-you-take-a-random-sample-from-a-large-dataset-in-python-and-r.html", "Q&A 20 How do you take a random sample from a large dataset in Python and R? 20.1 Explanation 20.2 Python Code 20.3 R Code", " Q&A 20 How do you take a random sample from a large dataset in Python and R? 20.1 Explanation Sampling is useful when working with large datasets, doing quick inspections, or preparing training/test splits. It allows you to explore a manageable portion of the data without loading everything. In this example, we’ll take a random sample of 500 rows from the diamonds dataset — which is considerably larger and more varied than the iris dataset we’ve used so far. 🎨 Why Switch to the Diamonds Dataset? The Iris dataset is a great starting point for learning structure, filtering, and variable creation — but it’s small and limited in variety. The Diamonds dataset is much larger and richer, with: Categorical variables like cut, color, and clarity Continuous variables like price, carat, and dimensions Greater potential for beautiful and insightful visualizations We’ll continue using Iris for clarity and comparison, but starting in the Visualization Layer, you’ll also work with a sampled Diamonds dataset to practice chart types like: Boxplots grouped by cut or clarity Scatter plots of carat vs. price Histograms of distribution patterns Sampling allows us to keep things fast and responsive while still leveraging the power of a large, visual-friendly dataset. 20.2 Python Code import pandas as pd import seaborn as sns # Load full diamonds dataset diamonds = sns.load_dataset(&quot;diamonds&quot;) # Take a random sample of 500 rows sampled_diamonds = diamonds.sample(n=500, random_state=42) # Save to CSV (optional) sampled_diamonds.to_csv(&quot;data/diamonds_sample.csv&quot;, index=False) # View first few rows print(sampled_diamonds.head()) carat cut color clarity depth table price x y z 1388 0.24 Ideal G VVS1 62.1 56.0 559 3.97 4.00 2.47 50052 0.58 Very Good F VVS2 60.0 57.0 2201 5.44 5.42 3.26 41645 0.40 Ideal E VVS2 62.1 55.0 1238 4.76 4.74 2.95 42377 0.43 Premium E VVS2 60.8 57.0 1304 4.92 4.89 2.98 17244 1.55 Ideal E SI2 62.3 55.0 6901 7.44 7.37 4.61 20.3 R Code library(ggplot2) library(dplyr) library(readr) # Load full diamonds dataset data(&quot;diamonds&quot;) # loads dataset into the environment # Access the dataset diamonds_df &lt;- diamonds # Take a random sample of 500 rows set.seed(42) diamonds_sample &lt;- sample_n(diamonds_df, 500) # Save to CSV (optional) write_csv(diamonds_sample, &quot;data/diamonds_sample.csv&quot;) # View first few rows head(diamonds_sample) # A tibble: 6 × 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.39 Ideal I VVS2 60.8 56 849 4.74 4.76 2.89 2 1.12 Very Good G SI2 63.3 58 4478 6.7 6.63 4.22 3 0.51 Very Good G VVS2 62.9 57 1750 5.06 5.12 3.2 4 0.52 Very Good D VS1 62.5 57 1829 5.11 5.16 3.21 5 0.28 Very Good E VVS2 61.4 55 612 4.22 4.25 2.6 6 1.01 Fair F SI1 67.2 60 4276 6.06 6 4.05 "],["eda-summary.html", "EDA Summary 📈 What Comes After EDA? 🚀 Continue Learning with CDI", " EDA Summary You’ve successfully completed the Exploratory Data Analysis (EDA) layer of the CDI Learning System — working hands-on in both Python and R to load, inspect, clean, transform, and save your data. This foundational layer has equipped you with the confidence to move forward — not just knowing how to use code, but how to think like a data scientist. 🧱 What You’ve Accomplished ✅ Set up a clean, organized project workspace ✅ Practiced essential EDA techniques with real data ✅ Standardized and saved a reusable dataset (data/iris.csv) ✅ Built fluency in both Python and R side by side 📈 What Comes After EDA? Now that you’ve completed the Exploratory Data Analysis (EDA) layer, you’re ready to move beyond structure and cleaning — and into data storytelling. The next stages of your journey include: 🎨 Data Visualization (VIZ) — turn data into clear, compelling visual insights 📐 Statistical Analysis (STATS) — test hypotheses and draw valid conclusions 🤖 Machine Learning (ML) — build predictive models using real-world data In these upcoming layers, you’ll continue working with familiar datasets — and explore new ones tailored to each domain. 🚀 Continue Learning with CDI Looking to go further or dive into another domain? 📚 Explore All CDI Products → ✅ With a strong analytical foundation, you’re now equipped to structure, inspect, and transform data confidently — preparing it for visualization, statistical analysis, and machine learning. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
